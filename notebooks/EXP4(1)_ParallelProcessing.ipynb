{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3caa5c60-38d8-40ac-8ff4-bbe03268c872",
   "metadata": {},
   "outputs": [],
   "source": [
    "%config Completer.use_jedi = False\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2fb9e60-d267-4415-8be1-c641396d5ef4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import os\n",
    "import torch\n",
    "import numpy as np \n",
    "import pandas as pd \n",
    "from glob import glob\n",
    "from tqdm import tqdm\n",
    "import supervision as sv\n",
    "import matplotlib.pyplot as plt\n",
    "from groundingdino.util.inference import Model\n",
    "from segment_anything import sam_model_registry, SamPredictor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ba10693-e7b3-43aa-9c85-91e566e254c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_image_with_mask(img, mask, alpha=0.4):\n",
    "    \"\"\"\n",
    "    Plots the image with an overlay of the mask in red, resizing the mask to match the image.\n",
    "\n",
    "    Args:\n",
    "    - img (str or numpy array): Path to the image file or image array.\n",
    "    - mask (str or numpy array): Path to the mask file or mask array.\n",
    "    - alpha (float): Transparency for overlaying mask.\n",
    "    \"\"\"\n",
    "\n",
    "    # If img is a path, read the image file\n",
    "    if isinstance(img, str):\n",
    "        img = cv2.imread(img)\n",
    "\n",
    "    # If mask is a path, read the mask file\n",
    "    if isinstance(mask, str):\n",
    "        mask = cv2.imread(mask)\n",
    "\n",
    "    # Convert image from BGR to RGB for displaying with matplotlib\n",
    "    img_rgb = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "    # Resize the mask to match the image size\n",
    "    mask_resized = cv2.resize(mask, (img_rgb.shape[1], img_rgb.shape[0]))\n",
    "\n",
    "    # Combine all three channels of the mask by taking the max value across channels\n",
    "    mask = np.clip(np.max(mask_resized, axis=2), 0, 1)\n",
    "\n",
    "    # Create a colored mask (Red where the mask is 1, transparent elsewhere)\n",
    "    colored_mask = np.zeros_like(img_rgb)  # Same shape as img, but all zeros\n",
    "    colored_mask[:, :, 0] = mask * 255  # Red channel gets the mask values\n",
    "\n",
    "    # Overlay the red mask on the original image using alpha blending\n",
    "    overlay_img = cv2.addWeighted(img_rgb, 1 - alpha, colored_mask, alpha, 0)\n",
    "\n",
    "    # Plot the image with the red mask overlay\n",
    "    plt.figure(figsize=(5, 5))\n",
    "    plt.imshow(overlay_img)\n",
    "    plt.title(\"Image with Red Mask Overlay\")\n",
    "    plt.axis('off')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ce4030d-b6f6-461f-b933-71512d815b00",
   "metadata": {},
   "outputs": [],
   "source": [
    "masks = glob(\"../data/dataset/training_noisy_labels/*png\")\n",
    "images = [i.replace(\"training_noisy_labels\", \"training_patches\") for i in masks]\n",
    "len(images) , len(masks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2c9cce4-3a50-4809-9008-19a69d118711",
   "metadata": {},
   "outputs": [],
   "source": [
    "idx = 100\n",
    "plot_image_with_mask(images[idx], masks[idx])\n",
    "images[idx], masks[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ccb3f6f-96e4-435e-a272-f4a2758f7033",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Grounding DINO\n",
    "CONFIG_PATH = \"../GroundingDINO/groundingdino/config/GroundingDINO_SwinT_OGC.py\"\n",
    "WEIGHTS_PATH = \"../weights/groundingdino_swint_ogc.pth\"\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee159e8f-23d0-4840-82d2-aa7eff3e2e93",
   "metadata": {},
   "outputs": [],
   "source": [
    "def segment(sam_predictor: SamPredictor, image: np.ndarray, xyxy: np.ndarray) -> np.ndarray:\n",
    "    sam_predictor.set_image(image)\n",
    "    result_masks = []\n",
    "    for box in xyxy:\n",
    "        masks, scores, logits = sam_predictor.predict(\n",
    "            box=box,\n",
    "            multimask_output=False\n",
    "        )\n",
    "        index = np.argmax(scores)\n",
    "        result_masks.append(masks[index])\n",
    "    return np.array(result_masks)\n",
    "\n",
    "SAM_ENCODER_VERSION = \"vit_b\"\n",
    "SAM_CHECKPOINT_PATH = \"../weights/sam_vit_b_01ec64.pth\"\n",
    "sam = sam_model_registry[SAM_ENCODER_VERSION](checkpoint=SAM_CHECKPOINT_PATH)\n",
    "sam_predictor = SamPredictor(sam)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd968ef6-a83c-41ea-ba51-7a65c7c3b9b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "classes = [\"skyscrapers\",\"buildings\" , \"house\", \"warehouses\" ,\"factory\" ,\"urban areas\"]\n",
    "BOX_TRESHOLD = 0.10\n",
    "TEXT_TRESHOLD = 0.10\n",
    "\n",
    "iou_list=[]\n",
    "gd_model = Model(CONFIG_PATH, WEIGHTS_PATH , device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ac34bb0-8c98-4aad-9160-604e97e7cbc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import cv2\n",
    "\n",
    "def calculate_iou(pred_mask, gt_mask):\n",
    "\n",
    "    if isinstance(pred_mask, str):\n",
    "        pred_mask = cv2.imread(pred_mask)\n",
    "\n",
    "    if isinstance(gt_mask, str):\n",
    "        gt_mask = cv2.imread(gt_mask)\n",
    "\n",
    "    gt_mask = torch.from_numpy(gt_mask)\n",
    "    pred_mask = torch.from_numpy(pred_mask)\n",
    "\n",
    "    intersection = torch.sum(gt_mask*pred_mask)\n",
    "    union = torch.sum(pred_mask) + torch.sum(gt_mask) - intersection\n",
    "\n",
    "    iou = intersection / (union + 1e-6 )\n",
    "\n",
    "    return iou.item()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a5a0352-557f-419f-8957-0a9f2b6e0f1c",
   "metadata": {},
   "source": [
    "***WITHOUT MULTIPROCESSING\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d611518-4f36-4b57-aa16-5399714a90c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "import os\n",
    "import supervision as sv\n",
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "classes = [\"skyscrapers\",\"buildings\"]\n",
    "BOX_TRESHOLD = 0.15\n",
    "TEXT_TRESHOLD = 0.15\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e004e659-f638-42ae-907b-d9dfb3441e68",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# for img_path , mask_path in tqdm(zip(images[:30],masks[:30]), total=len(images)):\n",
    "#     IMAGE_NAME = os.path.basename(img_path)\n",
    "#     image = cv2.imread(img_path)\n",
    "#     image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "#     detections  = gd_model.predict_with_classes(\n",
    "#         image=image,\n",
    "#         classes=classes,\n",
    "#         box_threshold=BOX_TRESHOLD,\n",
    "#         text_threshold=TEXT_TRESHOLD,\n",
    "#     )\n",
    "#     boxes = []\n",
    "#     for box in detections.xyxy:\n",
    "#         if ((box[2]-box[0])*(box[3]-box[1])<15000):\n",
    "#             boxes.append(box)\n",
    "#     detections.xyxy = np.array(boxes)\n",
    "#     detections.mask = segment(\n",
    "#         sam_predictor=sam_predictor,\n",
    "#         image=cv2.cvtColor(image, cv2.COLOR_BGR2RGB),\n",
    "#         xyxy=detections.xyxy\n",
    "#     )\n",
    "    \n",
    "#     pred_mask = detections.mask.astype(int)\n",
    "#     if pred_mask.shape[0]==0:\n",
    "#         pred_mask = np.zeros((image.shape[0],image.shape[1]))\n",
    "#     else:\n",
    "#         pred_mask = np.max(pred_mask , axis = 0)\n",
    "    \n",
    "#     pred_mask = np.stack([pred_mask]*3, axis=-1)\n",
    "#     # output_path = img_path.replace('training_patches', 'pred_masks_sam_dino')\n",
    "#     # os.makedirs(os.path.dirname(output_path), exist_ok=True)\n",
    "\n",
    "#     # cv2.imwrite(output_path, pred_mask)\n",
    "#     iou = calculate_iou(pred_mask , mask_path)\n",
    "#     img_iou = [IMAGE_NAME , iou]\n",
    "#     iou_list.append(img_iou)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba5ac2e8-6ef2-4989-9daf-002d45e2350c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_image(img_path, mask_path, sam_predictor, gd_model, classes, BOX_TRESHOLD, TEXT_TRESHOLD):\n",
    "    IMAGE_NAME = os.path.basename(img_path)\n",
    "    image = cv2.imread(img_path)\n",
    "    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "    # Perform prediction\n",
    "    detections = gd_model.predict_with_classes(\n",
    "        image=image,\n",
    "        classes=classes,\n",
    "        box_threshold=BOX_TRESHOLD,\n",
    "        text_threshold=TEXT_TRESHOLD,\n",
    "    )\n",
    "\n",
    "    # Filter boxes based on size threshold\n",
    "    boxes = [box for box in detections.xyxy if (box[2] - box[0]) * (box[3] - box[1]) < 15000]\n",
    "    detections.xyxy = np.array(boxes)\n",
    "\n",
    "    # Check if image_embeddings are properly computed\n",
    "    if sam_predictor.features is None:\n",
    "        raise ValueError(f\"Image embeddings are not computed for image: {img_path}\")\n",
    "\n",
    "    # Perform segmentation using mask predictor\n",
    "    detections.mask = segment(\n",
    "        sam_predictor=sam_predictor,\n",
    "        image=cv2.cvtColor(image, cv2.COLOR_BGR2RGB),\n",
    "        xyxy=detections.xyxy\n",
    "    )\n",
    "    \n",
    "    # Process the mask\n",
    "    pred_mask = detections.mask.astype(int)\n",
    "    if pred_mask.shape[0] == 0:\n",
    "        pred_mask = np.zeros((image.shape[0], image.shape[1]))\n",
    "    else:\n",
    "        pred_mask = np.max(pred_mask, axis=0)\n",
    "\n",
    "    pred_mask = np.stack([pred_mask] * 3, axis=-1)\n",
    "\n",
    "    # Compute IoU\n",
    "    iou = calculate_iou(pred_mask, mask_path)\n",
    "    \n",
    "    return IMAGE_NAME, iou\n",
    "\n",
    "# Parallel processing remains the same\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "313c4bb7-c771-41cd-bd0d-fb4f640b33e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parallel processing of images\n",
    "def parallel_process_images(images, masks, sam_predictor, gd_model, classes, BOX_TRESHOLD, TEXT_TRESHOLD, max_workers=6):\n",
    "    iou_list = []\n",
    "    futures = []\n",
    "    \n",
    "    with ThreadPoolExecutor(max_workers=max_workers) as executor:\n",
    "        for img_path, mask_path in zip(images[:30], masks[:30]):\n",
    "            futures.append(executor.submit(process_image, img_path, mask_path, sam_predictor, gd_model, classes, BOX_TRESHOLD, TEXT_TRESHOLD))\n",
    "        \n",
    "        for future in tqdm(as_completed(futures), total=len(futures)):\n",
    "            img_iou = future.result()\n",
    "            iou_list.append(img_iou)\n",
    "    \n",
    "    return iou_list\n",
    "\n",
    "# Call the parallel processing function\n",
    "iou_list = parallel_process_images(images, masks, sam_predictor, gd_model, classes, BOX_TRESHOLD, TEXT_TRESHOLD)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0831566b-144d-4f28-b7df-d1fa7ce3cb0a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
